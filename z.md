A tarefa de classificação extrema multirótulo (XMC) é difícil de resolver apenas com aprendizado em contexto, pois modelos de linguagem (LMs) podem não ter conhecimento prévio sobre as classes precisas ou como atribuí-las, e geralmente é inviável demonstrar todas as classes em um prompt [1, p. 1]. Para lidar com isso, o artigo propõe um programa geral chamado Infer–Retrieve–Rank (IReRa), que define interações em várias etapas entre LMs e recuperadores para resolver eficientemente esses problemas [1, p. 1]. Essa abordagem é implementada usando o modelo de programação DSPy, que especifica sistemas em contexto de forma declarativa, e utiliza otimizadores DSPy para ajustar o programa a conjuntos de dados específicos com poucas demonstrações [1, p. 1]. O programa primário de classificação extrema, otimizado separadamente para cada tarefa, atinge resultados de ponta em três benchmarks (HOUSE, TECH, TECHWOLF), e também obtém desempenho competitivo em um benchmark com características muito diferentes (BioDEX) [1, p. 1]. Diferentemente de trabalhos anteriores, esta solução não requer ajuste fino, é facilmente aplicável a novas tarefas, alivia a engenharia de prompts e necessita de apenas dezenas de exemplos rotulados [1, p. 1].

O programa Infer–Retrieve–Rank (IReRa) opera em três etapas [1, p. 2]:
1.  **Infer**: Um LM processa o documento de entrada e sugere um conjunto de termos aplicáveis [1, p. 2].
2.  **Retrieve**: Um recuperador relaciona cada termo previsto ao espaço de rótulos real [1, p. 2].
3.  **Rank**: Um LM é usado para reclassificar os rótulos recuperados [1, p. 2].

Crucialmente, este método utiliza um recuperador congelado e LMs congelados. A ideia principal do Infer–Retrieve–Rank é que um recuperador congelado pode ser muito mais flexível se o LM aprender em contexto como prever consultas relevantes e interpretar os resultados recuperados [1, p. 2]. Os LMs subjacentes, o recuperador e os prompts são considerados hiperparâmetros do programa IReRa, que podem ser ajustados automaticamente ou configurados facilmente [1, p. 2]. Utilizando apenas 10 entradas de treinamento não rotuladas e aproximadamente 50 exemplos de validação rotulados, um prompt few-shot para os dois componentes do LM é construído, usando um LM professor zero-shot com um prompt semente minimalista [1, p. 2]. A abstração de compilação do DSPy lida com isso eficientemente: ela pega a lógica do programa definida, a instancia com um LM professor, processa os exemplos de treinamento não rotulados, gera rótulos zero-shot para cada etapa do programa e seleciona os melhores rótulos para colocar em um prompt few-shot com base no desempenho de validação [1, p. 2]. Como o programa consiste em dois módulos em contexto, eles são construídos sequencialmente [1, p. 2]. Em experimentos, o módulo Infer é instanciado com um modelo Llama-2-7b-chat [1, p. 2], enquanto o modelo professor para bootstrapping é GPT-3.5 [1, p. 2]. O módulo Rank é instanciado e construído tanto por um modelo GPT-4 [1, p. 2]. Adaptar o Infer–Retrieve–Rank a um novo conjunto de dados pode ser tão simples quanto escrever um novo prompt semente zero-shot minimalista, configurar quais LMs usar e executar o procedimento de otimização [1, p. 3].

A abordagem DSPy visa substituir a engenharia de prompts manual por módulos componíveis e otimizadores automáticos [2, p. 1]. Em vez de prompts de string de formato livre, os programadores DSPy definem uma "assinatura" para especificar declarativamente o que um LM precisa fazer [2, p. 3]. Um módulo é então declarado com essa assinatura, que, ao ser chamado, constrói um prompt formatado de acordo com as entradas e saídas da assinatura, chama um LM com demonstrações (se houver) e retorna a previsão [2, p. 3]. O framework DSPy inclui módulos que encapsulam técnicas de prompting populares, como "Chain-of-Thought" [2, p. 3]. Esses módulos podem ser compostos em pipelines arbitrários, e os otimizadores do DSPy automatizam a geração de demonstrações de alta qualidade (exemplos few-shot) ou instruções para uma tarefa, dadas uma métrica a ser otimizada [2, p. 3]. Isso permite a compilação de pipelines de LM em cadeias auto-otimizadas de prompts ou ajustes finos [2, p. 3].

A introdução de "LM Assertions" no DSPy permite expressar restrições computacionais que os LMs devem satisfazer [3, p. 1]. Essas asserções podem ser "Assert" (restrições rígidas que, se violadas, interrompem o pipeline após tentativas de retri) ou "Suggest" (restrições mais brandas que incentivam o auto-refinamento, mas não param a execução se falharem após retri) [3, p. 3]. Essas asserções podem ser usadas em tempo de inferência para auto-refinamento (assertion-driven backtracking) e em tempo de compilação para otimizar prompts (assertion-driven example bootstrapping e counterexample bootstrapping) [3, p. 1]. Em avaliações, as LM Assertions melhoram a conformidade com regras e o desempenho em tarefas downstream, passando restrições até 164% mais vezes e gerando respostas de até 37% mais alta qualidade [3, p. 1]. Na tarefa de MultiHopQA, a introdução de duas sugestões simples melhora o recall do recuperador (em 6,5%–7,9%) e a precisão das respostas geradas (em 3,4%–14,4%) [3, p. 3]. No geral, as LM Assertions, quando integradas ao DSPy, aprimoram a confiabilidade, previsibilidade e correção da saída do pipeline de LM [3, p. 3].

---
**Referências**

[1] D’Oosterlinck, K., Khattab, O., Remy, F., Demeester, T., Develder, C., Potts, C. (2024). In-Context Learning for Extreme Multi-Label Classification. *arXiv preprint arXiv:2401.12178*. (p. 1-3)
[2] Khattab, O., Singhvi, A., Maheshwari, P., Zhang, Z., Santhanam, K., Vardhamanan, S., Haq, S., Sharma, A., Joshi, T. T., Moazam, H., Miller, H., Zaharia, M., Potts, C. (2024). DSPy: Compiling Declarative Language Model Calls into Self-Improving Pipelines. *arXiv preprint arXiv:2310.03714*. (p. 1-3)
[3] Singhvi, A., Shetty, M., Tan, S., Potts, C., Sen, K., Zaharia, M., Khattab, O. (2024). DSPy Assertions: Computational Constraints for Self-Refining Language Model Pipelines. *arXiv preprint arXiv:2312.13382*. (p. 1-3)